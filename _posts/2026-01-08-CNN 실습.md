---
layout: posts
title: "CNN 실습"
categories: ["DeepLearning"]
---

### CNN Basics 실습

**Image: 1, 3, 3, 1 image, Filter: 2, 2, 1, 1, Stride: 1x1, Padding: VALID**

<img width="802" height="836" alt="image" src="https://github.com/user-attachments/assets/ec8c07ca-cbe6-4211-bd74-693f44290382" />

**Image: 1, 3, 3, 1 image, Filter: 2, 2, 1, 1, Stride: 1x1, Padding: SAME**

Padding: SAME 옵션을 주게되면 output 이 input 이미지와 동일해진다.
<img width="1802" height="904" alt="image" src="https://github.com/user-attachments/assets/f0df8a87-10e2-4a57-889f-be4e551e4c8c" />
<img width="767" height="847" alt="image" src="https://github.com/user-attachments/assets/fcd1ef08-8b89-4c87-a220-779501476958" />

``` python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

image = np.array([[
    [[1],[2],[3]],
    [[4],[5],[6]],
    [[7],[8],[9]]
]], dtype=np.float32)  # (1,3,3,1)

kernel = np.array([
    [[[1, 10, -1]], [[1, 10, -1]]],
    [[[1, 10, -1]], [[1, 10, -1]]]
], dtype=np.float32)   # (2,2,1,3)

conv = tf.nn.conv2d(image, kernel, strides=[1,1,1,1], padding='SAME')
out = conv.numpy()

print("image:", image.shape)     # (1, 3, 3, 1)
print("kernel:", kernel.shape)   # (2, 2, 1, 3)
print("out:", out.shape)         # (1, 3, 3, 3)

for c in range(out.shape[-1]):
    print(f"\nchannel {c}:\n", out[0, :, :, c])

plt.figure(figsize=(9,3))
for c in range(out.shape[-1]):
    plt.subplot(1, 3, c+1)
    plt.title(f"ch {c}")
    plt.imshow(out[0,:,:,c], cmap='gray')
plt.show()
```
<img width="760" height="271" alt="image" src="https://github.com/user-attachments/assets/d851897d-7b23-4dd9-b1c2-0bb2ea7a0adf" />

``` python
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

mnist = tfds.load("mnist", split="train")

for ex in mnist.take(1):
    img0 = ex["image"].numpy().squeeze()   # (28,28)
    label = ex["label"].numpy()

plt.title(f"label = {label}")
plt.imshow(img0, cmap="gray")
plt.axis("off")
plt.show()

# (-1,28,28,1) + 정규화
img = img0.reshape(-1, 28, 28, 1).astype("float32") / 255.0

# 3x3 의 필터 5개, 1 = channel 수
W1 = tf.Variable(tf.random.normal([3, 3, 1, 5], stddev=0.01))
conv = tf.nn.conv2d(img, W1, strides=[1, 2, 2, 1], padding="SAME")
conv = tf.nn.relu(conv)

print(conv.shape)  # (1, 14, 14, 5)

acts = conv.numpy()[0]  # (14,14,5)

plt.figure(figsize=(10, 2))
for i in range(acts.shape[-1]):  # 5개
    plt.subplot(1, 5, i + 1)
    plt.imshow(acts[:, :, i], cmap="gray")
    plt.axis("off")
plt.show()
```
<img width="389" height="411" alt="image" src="https://github.com/user-attachments/assets/7788be9c-2b6e-4bb7-8b69-adcac693f92f" />
<img width="795" height="153" alt="image" src="https://github.com/user-attachments/assets/3e2c53f2-19d8-439e-adee-6d7e789880eb" />

``` python
import tensorflow as tf
from tensorflow import keras
from keras import layers
import matplotlib.pyplot as plt
import numpy as np

# 1) 모델 정의 (레이어 이름을 명시해두면 get_layer가 안전함)
X = keras.Input(shape=(784,), dtype="float32")
x = layers.Reshape((28, 28, 1), name="reshape")(X)

# block 1
x = layers.Conv2D(32, 3, strides=1, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv1")(x)
p1 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool1")(x)  # (None,14,14,32)

# block 2
x = layers.Conv2D(64, 3, strides=2, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv2")(p1)
p2 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool2")(x)  # (None,4,4,64)

# fully connected
flat = layers.Flatten(name="flatten")(p2)        # (None, 1024)
logits = layers.Dense(10, name="logits")(flat)   # (None, 10)

model = keras.Model(inputs=X, outputs=logits, name="mnist_cnn")
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.summary()

# 2) 중간 feature map 뽑는 모델 (pool1, pool2 같이 출력)
feat_model = keras.Model(inputs=X, outputs=[p1, p2, logits], name="feat_model")

# 3) MNIST 로드/전처리
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype("float32") / 255.0
x_test  = x_test.reshape(-1, 784).astype("float32") / 255.0

# 4) 학습
model.fit(x_train, y_train, batch_size=128, epochs=1, validation_split=0.1)

# 5) 테스트 1장으로 중간 출력/예측 보기
sample = x_test[0:1]  # (1,784)
p1_out, p2_out, logits_out = feat_model(sample)

p1_out = p1_out.numpy()      # (1,14,14,32)
p2_out = p2_out.numpy()      # (1,4,4,64)
logits_out = logits_out.numpy()[0]  # (10,)
probs = tf.nn.softmax(logits_out).numpy()

# 원본 이미지
img = sample.reshape(28, 28)
plt.imshow(img, cmap="gray")
plt.title(f"true label = {y_test[0]}")
plt.axis("off")
plt.show()

# pool1 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p1_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool1: (14,14,32) first 8 channels")
plt.show()

# pool2 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p2_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool2: (4,4,64) first 8 channels")
plt.show()

# 최종 예측은 이미지가 아니라 벡터(10개)라서 bar plot이 맞음
plt.bar(range(10), probs)
plt.xticks(range(10))
plt.ylim(0, 1)
plt.title(f"pred = {np.argmax(probs)}")
plt.show()
```
<img width="389" height="411" alt="image" src="https://github.com/user-attachments/assets/ee47086f-d153-4aea-8d89-73a63c9e2ddb" />

<img width="950" height="166" alt="image" src="https://github.com/user-attachments/assets/50e85c75-5b9b-4f16-a682-3ba24ef17047" />

<img width="950" height="166" alt="image" src="https://github.com/user-attachments/assets/d443c62b-b694-41b7-ac2e-737d8bec8cfd" />

<img width="547" height="435" alt="image" src="https://github.com/user-attachments/assets/f2eafadf-b425-4bd8-a828-1b46f8f01c4a" />

``` python
import tensorflow as tf
from tensorflow import keras
from keras import layers
import matplotlib.pyplot as plt
import numpy as np

# 1) 모델 정의
X = keras.Input(shape=(784,), dtype="float32")
x = layers.Reshape((28, 28, 1), name="reshape")(X)

# block 1
x = layers.Conv2D(32, 3, strides=1, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv1")(x)
p1 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool1")(x)  # (?,14,14,32)

# block 2
x = layers.Conv2D(64, 3, strides=1, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv2")(p1)
p2 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool2")(x)  # (?,7,7,64)

# block 3
x = layers.Conv2D(128, 3, strides=1, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv3")(p2)
p3 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool3")(x)  # (?,4,4,128)

# fully connected (사진처럼 625 hidden layer 추가)
flat = layers.Flatten(name="flatten")(p3)             # (?, 4*4*128 = 2048)
fc1  = layers.Dense(625, activation="relu", name="fc1")(flat)  # (?, 625)
logits = layers.Dense(10, name="logits")(fc1)         # (?, 10)

model = keras.Model(inputs=X, outputs=logits, name="mnist_cnn")
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.summary()

feat_model = keras.Model(inputs=X, outputs=[p1, p2, p3, logits], name="feat_model")

# 2) MNIST 로드/전처리
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype("float32") / 255.0
x_test  = x_test.reshape(-1, 784).astype("float32") / 255.0

# 3) 학습
model.fit(x_train, y_train, batch_size=128, epochs=1, validation_split=0.1)

sample = x_test[3:4]
p1_out, p2_out, p3_out, logits_out = feat_model(sample)

p1_out = p1_out.numpy()  # (1,14,14,32)
p2_out = p2_out.numpy()  # (1,7,7,64)
p3_out = p3_out.numpy()  # (1,4,4,128)

logits_vec = logits_out.numpy()[0]        # (10,)
probs = tf.nn.softmax(logits_vec).numpy()

# 원본 이미지
img = sample.reshape(28, 28)
plt.imshow(img, cmap="gray")
plt.title(f"true label = {y_test[0]}")
plt.axis("off")
plt.show()

# pool1 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p1_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool1: (14,14,32) first 8 channels")
plt.show()

# pool2 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p2_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool2: (7,7,64) first 8 channels")
plt.show()

# pool3 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p3_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool3: (4,4,128) first 8 channels")
plt.show()

# 최종 예측 확률
plt.bar(range(10), probs)
plt.xticks(range(10))
plt.ylim(0, 1)
plt.title(f"pred = {np.argmax(probs)}")
plt.show()
```
<img width="389" height="411" alt="image" src="https://github.com/user-attachments/assets/04eb1c84-297e-4641-84c2-d3997fe9344e" />

<img width="950" height="166" alt="image" src="https://github.com/user-attachments/assets/45872be7-6ab5-4370-9d2e-2d88a046686a" />

<img width="950" height="166" alt="image" src="https://github.com/user-attachments/assets/968d61d1-772d-4328-801f-b16f6dfa28a7" />

<img width="950" height="166" alt="image" src="https://github.com/user-attachments/assets/1ba98566-1318-4aec-9a91-e2699aecc3d8" />

<img width="547" height="435" alt="image" src="https://github.com/user-attachments/assets/ad75fd47-ecf3-4e57-96e4-95260fa6996b" />


나를 위한 주석 추가

``` python
import tensorflow as tf
from tensorflow import keras
from keras import layers
import matplotlib.pyplot as plt
import numpy as np

"""
[Conv2D 개념]
- Conv2D는 작은 필터(커널)가 이미지를 훑으며(feature extraction) 특징을 뽑는 연산
- filters=32: 서로 다른 필터 32개를 학습(=출력 채널 32개)
- kernel_size=3: 필터 크기 3x3
- strides=1: 한 칸씩 이동
- padding="same": 출력의 height/width가 입력과 같도록 가장자리에 패딩을 넣음
- activation="relu": 음수는 0으로 (비선형성 추가)
"""

# 1) 모델 정의
X = keras.Input(shape=(784,), dtype="float32")
x = layers.Reshape((28, 28, 1), name="reshape")(X)

# block 1
x = layers.Conv2D(32, 3, strides=1, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv1")(x)
p1 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool1")(x)  # (?,14,14,32)

"""
[MaxPooling2D 개념]
- 풀링은 "요약/다운샘플링" 역할
- pool_size=2, strides=2:
  2x2 영역에서 최대값만 남기고,
  두 칸씩 이동하면서 height/width를 대략 절반으로 줄임
- padding="same"이면 홀수 크기에서도 크기 계산이 ceil 형태로 됨
"""

# block 2
x = layers.Conv2D(64, 3, strides=1, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv2")(p1)
p2 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool2")(x)  # (?,7,7,64)

# block 3
x = layers.Conv2D(128, 3, strides=1, padding="same",
                  use_bias=False,
                  kernel_initializer=keras.initializers.RandomNormal(stddev=0.01),
                  activation="relu",
                  name="conv3")(p2)
p3 = layers.MaxPooling2D(2, strides=2, padding="same", name="pool3")(x)  # (?,4,4,128)

# fully connected
flat = layers.Flatten(name="flatten")(p3)             # (?, 4*4*128 = 2048)
fc1  = layers.Dense(625, activation="relu", name="fc1")(flat)  # (?, 625)
     
"""
[Dense/FC 개념]
- Dense(완전연결층)는 각 입력 특징과 출력 뉴런을 모두 연결
- units=625는 "설계자가 정한 은닉층 크기(하이퍼파라미터)"
  -> 사진 코드에서 625를 썼기 때문에 그대로 맞춤
- 마지막 Dense(10)은 MNIST 0~9 = 클래스 10개
"""
logits = layers.Dense(10, name="logits")(fc1)   

"""
[logits vs softmax 확률]
- logits: softmax를 통과하기 전의 "점수(score)" (실수값, 합이 1일 필요 없음)
- softmax(logits): 각 클래스 확률(0~1), 합이 1

[loss에서 from_logits=True 의미]
- SparseCategoricalCrossentropy(from_logits=True)는
  내부에서 softmax를 안정적으로 계산한 뒤 cross-entropy를 구함
- 그래서 모델 출력은 logits로 두고, loss에 from_logits=True로 주는 게 흔한 방식
"""

model = keras.Model(inputs=X, outputs=logits, name="mnist_cnn")
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.summary()

feat_model = keras.Model(inputs=X, outputs=[p1, p2, p3, logits], name="feat_model")

# 2) MNIST 로드/전처리
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype("float32") / 255.0
x_test  = x_test.reshape(-1, 784).astype("float32") / 255.0

# 3) 학습
model.fit(x_train, y_train, batch_size=128, epochs=1, validation_split=0.1)

sample = x_test[3:4]
p1_out, p2_out, p3_out, logits_out = feat_model(sample)

p1_out = p1_out.numpy()  # (1,14,14,32)
p2_out = p2_out.numpy()  # (1,7,7,64)
p3_out = p3_out.numpy()  # (1,4,4,128)

logits_vec = logits_out.numpy()[0]        # (10,)
probs = tf.nn.softmax(logits_vec).numpy()

# 원본 이미지
img = sample.reshape(28, 28)
plt.imshow(img, cmap="gray")
plt.title(f"true label = {y_test[0]}")
plt.axis("off")
plt.show()

# pool1 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p1_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool1: (14,14,32) first 8 channels")
plt.show()

# pool2 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p2_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool2: (7,7,64) first 8 channels")
plt.show()

# pool3 feature map 8개
plt.figure(figsize=(12, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(p3_out[0, :, :, i], cmap="gray")
    plt.axis("off")
plt.suptitle("pool3: (4,4,128) first 8 channels")
plt.show()

# 최종 예측 확률
plt.bar(range(10), probs)
plt.xticks(range(10))
plt.ylim(0, 1)
plt.title(f"pred = {np.argmax(probs)}")
plt.show()


```
