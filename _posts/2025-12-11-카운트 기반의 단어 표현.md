---
layout: posts
title: "카운트 기반의 단어 표현"
categories: ["DeepLearning"]
---


### 카운트 기반의 단어 표현

- 다양한 단어의 표현 방법
    - 국소 표현 (이산 표현)
        - 각 단어에 1, 2, 3 등과 같은 숫자를 맵핑하여 부여한다.
    - 분산 표현 (연속 표현)
        - 해당 단어를 표현하기 위해 주변 단어를 참고한다.
- 단어의 카테고리화
    
    
- Bag of Words
    - 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법이다.
        <img width="511" height="273" alt="image" src="https://github.com/user-attachments/assets/8851c4e2-8323-4be8-a08c-8f15c4f9de90" />

        ```python
        #!pip install konlpy
        from konlpy.tag import Okt
        
        okt = Okt()
        
        def build_bag_of_words(document):
          # 온점 제거 및 형태소 분석
          document = document.replace('.', '')
          tokenized_document = okt.morphs(document)
        
          word_to_index = {}
          bow = []
        
          for word in tokenized_document:  
            if word not in word_to_index.keys():
              word_to_index[word] = len(word_to_index)  
              # BoW에 전부 기본값 1을 넣는다.
              bow.insert(len(word_to_index) - 1, 1)
            else:
              # 재등장하는 단어의 인덱스
              index = word_to_index.get(word)
              # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.
              bow[index] = bow[index] + 1
        
          return word_to_index, bow
          
        doc1 = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
        vocab, bow = build_bag_of_words(doc1)
        print('vocabulary :', vocab)
        print('bag of words vector :', bow)
        
        vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}
        bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]
        ```
        
    - BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이므로 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 데 주로 사용된다. 즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 사용된다.
- CountVectorizer 클래스로 BoW 만들기
    
    ```python
    from sklearn.feature_extraction.text import CountVectorizer
    
    corpus = ['you know I want your love. because I love you.']
    vector = CountVectorizer()
    
    # 코퍼스로부터 각 단어의 빈도수를 기록
    print('bag of words vector :', vector.fit_transform(corpus).toarray()) 
    
    # 각 단어의 인덱스가 어떻게 부여되었는지를 출력
    print('vocabulary :',vector.vocabulary_)
    
    # 결과
    bag of words vector : [[1 1 2 1 2 1]]
    vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}
    ```
    
- 불용어
    
    ```python
    text = ["Family is not an important thing. It's everything."]
    vect = CountVectorizer(stop_words=["the", "a", "an", "is", "not"])
    print('bag of words vector :',vect.fit_transform(text).toarray())
    print('vocabulary :',vect.vocabulary_)
    
    # 결과
    bag of words vector : [[1 1 1 1 1]]
    vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}
    ```
    
- 문서 단어 행렬
    
    ⇒ 각 문서에서 등장한 단어의 빈도를 행렬의 값으로 표기한다. 문서 단어 행렬은 문서들을 서로 비교할 수 있도록 수치화할 수 있다는 점에서 의의를 갖는다. 만약 필요에 따라서는 형태소 분석기로 단어 토큰화를 수행하고, 불용어에 해당되는 조사들 또한 제거하여 더 정제된 DTM을 만들수도 있다.
    
    - 문서 단어 행렬(Document-Term Matrix)의 한계
        - 희소 표현
            - DTM 에서의 각 행을 문서 벡터라고 할 때, 각 문서 벡터의 차원은 원-핫 벡터와 마찬가지로 전체 단어 집합의 크기를 가진다.
            - 이 때문에 전처리를 통해 단어 집합의 크기를 줄이는 일은 BoW 표현을 사용하는 모델에서 중요할 수 있다.
        - 단순 빈도 수 기반 접근
- **TF-IDF (단어 빈도-역 문서 빈도)**
    - 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법이다.
    - 우선 DTM 을 만든 후, TF-IDF 가중치를 부여한다.
    - **tf(d, t)**: **특정 문서 d에서의 특정 단어 t의 등장 횟수**
    - **df(t)**: 특정 단어 t가 등장한 **문서의 수**
    - **idf(t)**: df(t)에 반비례하는 수
        
        ![image.png](attachment:5bcc4fd2-e57a-4a16-a5dd-43a16a9af0ea:image.png)
        
        ![image.png](attachment:900fa1d9-7bd6-4def-9781-04e00b62f825:image.png)
        
        - log 사용 이유
            
            df 의 역수를 그대로 사용한다면 총 문서의 수 n이 커질수록, IDF 의 값은 기하급수적으로 커지게 된다. 그렇기 때문에 log를 사용한다.
            
            불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장한다. 그런데 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 또 최소 수백 밴 더 자주 등장하는 편이다. 이때문에 log 를 씌워주지 않으면, 희귀 단어들에 엄청난 가중치가 부여될 수 있다.
            
    - TF-IDF 는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단한다.
    - TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것이다. 즉, the 나 a 와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF 의 값은 다른 단어의 TF-IDF 에 비해서 낮아지게 된다.
    - TF-IDF 의 로그는 대부분 자연 로그를 사용한다.
        
        ```python
        import pandas as pd # 데이터프레임 사용을 위해
        from math import log # IDF 계산을 위해
        
        docs = [
          '먹고 싶은 사과',
          '먹고 싶은 바나나',
          '길고 노란 바나나 바나나',
          '저는 과일이 좋아요'
        ] 
        vocab = list(set(w for doc in docs for w in doc.split()))
        vocab.sort()
        ```
        
        ```python
        # 총 문서의 수
        N = len(docs) 
        
        def tf(t, d):
          return d.count(t)
        
        def idf(t):
          df = 0
          for doc in docs:
            df += t in doc
          return log(N/(df+1))
        
        def tfidf(t, d):
          return tf(t,d)* idf(t)
        ```
        
        ```python
        result = []
        
        # 각 문서에 대해서 아래 연산을 반복
        for i in range(N):
          result.append([])
          d = docs[i]
          for j in range(len(vocab)):
            t = vocab[j]
            result[-1].append(tf(t, d))
        
        tf_ = pd.DataFrame(result, columns = vocab)
        tf_
        ```
        
        ```python
        result = []
        for j in range(len(vocab)):
            t = vocab[j]
            result.append(idf(t))
        
        idf_ = pd.DataFrame(result, index=vocab, columns=["IDF"])
        idf_
        ```
        
        ```python
        result = []
        for i in range(N):
          result.append([])
          d = docs[i]
          for j in range(len(vocab)):
            t = vocab[j]
            result[-1].append(tfidf(t,d))
        
        tfidf_ = pd.DataFrame(result, columns = vocab)
        tfidf_
        ```
        
    - 실제 TF-IDF 구현을 제공하고 있는 많은 머신 러닝 패키지들은 패키지마다 식이 상이하다. 이유는 만약 전체 문서의 수 n이 4인데 df(t) 의 값이 3인 경우에는 log 항의 분자와 분모의 값이 같아지게 된다. IDF 의 값이 0이라면 더 이상 가중치의 역할을 수행하지 못한다.
