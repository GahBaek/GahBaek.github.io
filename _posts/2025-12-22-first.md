layout: posts
title: "ReLU, weights (lec 9-10)"
categories: ["DeepLearning"]

### Backpropagation

이전 학습에서 2개의 unit 이 이어진 네트워크를 이용해서 XOR 문제를 해결할 수 있다는 것을 알아냈는데 이 네트워크의 가중치와 편향은 어떻게 학습할 수 있을까 ?

<img width="426" height="283" alt="image" src="https://github.com/user-attachments/assets/82fd58de-a139-451b-b091-d1eed7c0a0bb" />

w, x, b 각각이 f 에 미치는 영향

1. forward → 실제 값을 입력
2. backward → 미분의 값을 구한다.

**chain rule**

```python
# 단층 퍼셉트론의 한계
# 단층 퍼셉트론의 경우 입력 공간을 하나의 직선으로만 나눌 수 있지만
# XOR 은 하나의 직선으로 표현할 수 없다.
import tensorflow as tf
import numpy as np

# 1. 데이터 설정
x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)

# 2. 가중치 및 편향 (Variable로 선언)
W = tf.Variable(tf.random.normal([2, 1]))
b = tf.Variable(tf.random.normal([1]))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

for step in range(10001):
    with tf.GradientTape() as tape:
        # 가설 함수: 단층 퍼셉트론
        hypothesis = tf.sigmoid(tf.matmul(x_data, W) + b)
        cost = -tf.reduce_mean(y_data * tf.math.log(hypothesis + 1e-7) + 
                              (1 - y_data) * tf.math.log(1 - hypothesis + 1e-7))

    # 미분 계산 및 최적화
    gradients = tape.gradient(cost, [W, b])
    optimizer.apply_gradients(zip(gradients, [W, b]))

    if step % 1000 == 0:
        predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_data), dtype=tf.float32))
        print(f"Step: {step:5} | Cost: {cost.numpy():.4f} | Accuracy: {accuracy.numpy():.2f}")

# 결과 확인
print("\n[최종 예측 결과]")
print(hypothesis.numpy())
```

```python
# 단층 퍼셉트론의 한계를 넘어서 XOR 문제를 해결하기 위해 네트워크를 확장하는 방법
# 1. Wide

# Wide: 은닉층의 노드 수를 10개로 확장
W1 = tf.Variable(tf.random.normal([2, 10])) # 입력 2 -> 은닉층 노드 10
b1 = tf.Variable(tf.random.normal([10]))

W2 = tf.Variable(tf.random.normal([10, 1])) # 은닉층 노드 10 -> 출력 1
b2 = tf.Variable(tf.random.normal([1]))

# 가설 계산부 (GradientTape 내부)
layer1 = tf.sigmoid(tf.matmul(x_data, W1) + b1)
hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)

# 2. Deep

# Deep: 3개의 층으로 구성
W1 = tf.Variable(tf.random.normal([2, 5])) # 1층
b1 = tf.Variable(tf.random.normal([5]))

W2 = tf.Variable(tf.random.normal([5, 5])) # 2층
b2 = tf.Variable(tf.random.normal([5]))

W3 = tf.Variable(tf.random.normal([5, 1])) # 3층 (출력)
b3 = tf.Variable(tf.random.normal([1]))

# 가설 계산부 (GradientTape 내부)
l1 = tf.sigmoid(tf.matmul(x_data, W1) + b1)
l2 = tf.sigmoid(tf.matmul(l1, W2) + b2)
hypothesis = tf.sigmoid(tf.matmul(l2, W3) + b3)
```

- Matplotlib 를 이용한 학습 곡선 시각화
    - 학습 중에 cost 값을 리스트에 담아 두었다가 마지막에 선 그래프로 그리는 방식

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt  # 시각화 라이브러리 추가

# 데이터 설정
x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)

# 가중치 설정 (Wide & Deep)
W1 = tf.Variable(tf.random.normal([2, 10]))
b1 = tf.Variable(tf.random.normal([10]))
W2 = tf.Variable(tf.random.normal([10, 1]))
b2 = tf.Variable(tf.random.normal([1]))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

# 손실 값을 저장할 리스트
cost_history = []

for step in range(5001):
    with tf.GradientTape() as tape:
        l1 = tf.sigmoid(tf.matmul(x_data, W1) + b1)
        hypothesis = tf.sigmoid(tf.matmul(l1, W2) + b2)
        cost = -tf.reduce_mean(y_data * tf.math.log(hypothesis + 1e-7) + 
                              (1 - y_data) * tf.math.log(1 - hypothesis + 1e-7))

    gradients = tape.gradient(cost, [W1, b1, W2, b2])
    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))
    
    # 리스트에 현재 cost 저장
    cost_history.append(cost.numpy())

    if step % 1000 == 0:
        print(f"Step: {step:5} | Cost: {cost.numpy():.4f}")

# 그래프 그리기
plt.plot(cost_history)
plt.title('Model Cost over Steps')
plt.xlabel('Step')
plt.ylabel('Cost')
plt.show()
```

ReLU

- Backpropagation
    - 얕은 모델 (2~3단): 출력층에서 계산된 오차가 입력층까지 비교적 잘 전달되어 학습이 원활합니다.
    - 깊은 모델 (Deep Layer): 레이어를 층층이 쌓을수록, 입력층에 가까운 앞부분 레이어들의 가중치(Weight)가 업데이트되지 않고 초기 상태에 머무는 현상이 발생합니다.
    
    = Vanishing gradient
        

**“We used the wrong type of non-linearity”**

<img width="668" height="324" alt="image" src="https://github.com/user-attachments/assets/29a079c6-4696-411e-b305-3eceb86535e1" />

**Rectified Linear Unit**

: 입력값이 0보다 작으면 0이고 0보다 크면 입력값 그대로를 내보낸다.

마지막 단의 출력은 sigmoid 를 사용해줘야 한다. 0~1  사이의 값이어야하기 때문이다.

**activation function**

tanh, ELU 등

“We initialized the weights in a stupid way”

지금까지는 weight 값을 random 으로 주고 있었다.

→ 초기화를 잘 해보자.

**restricted boatman machine**

(지금은 잘 사용하지 않음)

forward 의 입력값과 backward 의 output 의 차가 최소가 되도록 (비슷한 값이 나오도록)

RBM 이후,

Xavier initialization, HE’s initialization

가중치 행렬 생성 시, fan_in, fan_out 의 개수를 사용한다.  
```python
// Xavier
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
// HE's
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)
```
