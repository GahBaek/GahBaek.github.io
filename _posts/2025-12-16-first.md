---
layout: posts
title: "multi-variable regression & classification (lec 4-7)"
categories: ["DeepLearning"]
---


**Recap**

- Hypothesis
- Cost function
- Gradient descent algorithm

**Multi-variable**

H(x) = Wx + b

$H(x1, x2, x3) = W1x1 + W2x2 + W3x3 + b$

**Matrix**

<img width="773" height="149" alt="image" src="https://github.com/user-attachments/assets/7728b624-ff44-4965-8151-dd1d259e2e1e" />


$H(x) = XW$

<img width="825" height="308" alt="image" src="https://github.com/user-attachments/assets/b26efaca-a620-4785-af23-b352beb2669d" />

**Multi-variable linear regression 실습**

```python
import tensorflow as tf

x1_data = tf.constant([73., 93., 89., 96., 73.], dtype=tf.float32)
x2_data = tf.constant([80., 88., 91., 98., 66.], dtype=tf.float32)
x3_data = tf.constant([75., 93., 90., 100., 70.], dtype=tf.float32)
y_data  = tf.constant([152., 185., 180., 196., 142.], dtype=tf.float32)

w1 = tf.Variable(tf.random.normal([1]), name='weight1')
w2 = tf.Variable(tf.random.normal([1]), name='weight2')
w3 = tf.Variable(tf.random.normal([1]), name='weight3')
b  = tf.Variable(tf.random.normal([1]), name='bias')

learning_rate = 0.00001  

print("--- 학습 시작 ---")
for step in range(2001): # 2000번 반복
    with tf.GradientTape() as tape:
        hypothesis = w1 * x1_data + w2 * x2_data + w3 * x3_data + b
        
        # 비용(Cost): 평균 제곱 오차
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    # 4. 기울기(Gradients) 계산
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])

    # 5. 변수 업데이트 (w = w - learning_rate * gradient)
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    # 100번마다 로그 출력
    if step % 100 == 0:
        print(f"Step: {step:4d}, Cost: {cost.numpy():.4f}")

print("--- 학습 완료 ---")
print(f"최종 결과: w1={w1.numpy()[0]:.2f}, w2={w2.numpy()[0]:.2f}, w3={w3.numpy()[0]:.2f}, b={b.numpy()[0]:.2f}")
```

```python
--- 학습 시작 ---
Step:    0, Cost: 27794.4648
Step:  100, Cost: 43.8701
Step:  200, Cost: 41.8470
Step:  300, Cost: 39.9293
Step:  400, Cost: 38.1112
Step:  500, Cost: 36.3879
Step:  600, Cost: 34.7542
Step:  700, Cost: 33.2052
Step:  800, Cost: 31.7366
Step:  900, Cost: 30.3443
Step: 1000, Cost: 29.0239
Step: 1100, Cost: 27.7720
Step: 1200, Cost: 26.5847
Step: 1300, Cost: 25.4589
Step: 1400, Cost: 24.3910
Step: 1500, Cost: 23.3782
Step: 1600, Cost: 22.4177
Step: 1700, Cost: 21.5064
Step: 1800, Cost: 20.6421
Step: 1900, Cost: 19.8220
Step: 2000, Cost: 19.0439
--- 학습 완료 ---
최종 결과: w1=-0.70, w2=0.73, w3=1.97, b=0.07
```

```python
# matrix 를 사용한 multi-variable linear regression
# 데이터 (5행 3열 행렬)
x_data = [
    [73., 80., 75.],
    [93., 88., 93.],
    [89., 91., 90.],
    [96., 98., 100.],
    [73., 66., 70.]
]
y_data = [[152.], [185.], [180.], [196.], [142.]] # (5행 1열)

# W는 [3, 1] 모양의 행렬 하나로 정의
W = tf.Variable(tf.random.normal([3, 1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')

# 학습 루프 안에서:
with tf.GradientTape() as tape:
    hypothesis = tf.matmul(x_data, W) + b  # 행렬 곱셈으로 한 번에 계산!
    cost = tf.reduce_mean(tf.square(hypothesis - y_data))

grads = tape.gradient(cost, [W, b])
```

**Logistic (regression) classification**

- regression
    - 값을 예측하는 것
- classification
    - 분류하는 것

**Classification Hypothesis**

<img width="524" height="205" alt="image" src="https://github.com/user-attachments/assets/d273af72-4e73-4519-ab1e-88a1c9fb36c8" />

→ regression 에서 사용한 H(x) = Wx 를 그대로 사용하기에는 문제가 있다.

이에 Sigmoid 를 사용한다.

<img width="485" height="323" alt="image" src="https://github.com/user-attachments/assets/75b7c16d-f92e-4054-a725-fb412b7ae148" />

**`Sigmoid`** = **`logistic function`** = **`sigmoid function`**

z 가 커지면 g(z) 험수수는 1 에 가까워짐

z 가 작아지면 g(z) 함수는 0 에 가까워짐

z = WX

<img width="333" height="111" alt="image" src="https://github.com/user-attachments/assets/b7816ef5-9f0f-4fbf-94b5-a91409b7ef9b" />

**Classification Cost Function**

<img width="485" height="285" alt="image" src="https://github.com/user-attachments/assets/57ee04d8-e5ba-430f-8f7d-b4c0c697212d" />

minimum 이 되는 구간이 여러가지이다. → regresssion 에서 사용한 cost 함수를 바로 적용하면 안된다.

$C(H(x), y) =$

- y = 1
    - $-log(H(x)$
- y = 0
    - $-log(1-H(x))$

```python
y = 1
H(x) = 1 -> cost(1) = 0
H(x) = 0 -> cost = ∞

y = 0
H(x) = 1 -> cost = ∞
H(x) = 0 -> cost(0) = 0
```

<img width="720" height="403" alt="image" src="https://github.com/user-attachments/assets/f219bdec-edb1-4335-bc57-7d41995a3d9e" />

if 문을 빼서

**$c(H(x), y) = -ylog(H(x)) - (1-y)log(1-H(x))$**

**Softmax Classification 실습**

- One-hot encoding
    
    ### 원-핫 인코딩
    
    컴퓨터 또는 기계는 문자보다 숫자를 더 잘 처리할 수 있다. 이를 위해 자연어 처리에서는 문자를 숫자로 바꾸는 여러 가지 기법이 있다.
    
    원-핫 인코딩은 그 많은 기법 중에서 단어를 표현하는 가장 기본적인 표현 방법이다.
    
    - one-hot encoding
        - 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다.
        - 과정은 다음과 같다.
            - 정수 인코딩을 수행한다.
            - 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고 다른 단어의 인덱스의 위치에는 0을 부여한다.
        - 장점
            - 단순하고 직관적
            - 순서 정보를 넣지 않는다.
        - 단점
            - **차원이 너무 커진다.** (고차원 + 희소 벡터)
            - 단어 사이의 유**사도 정보가 없다.**
            
            ⇒ 최근에는 임베딩을 더 사용한다.
            

```python
import tensorflow as tf

# 1. 데이터 정의 (float32로 변환 필수!)
x_data = tf.constant([
    [1, 2, 1, 1], 
    [2, 1, 3, 2], 
    [3, 1, 3, 4], 
    [4, 1, 5, 5], 
    [1, 7, 5, 5], 
    [1, 2, 5, 6], 
    [1, 6, 6, 6], 
    [1, 7, 7, 7]
], dtype=tf.float32)

y_data = tf.constant([
    [0, 0, 1], 
    [0, 0, 1], 
    [0, 0, 1], 
    [0, 1, 0], 
    [0, 1, 0], 
    [0, 1, 0], 
    [1, 0, 0], 
    [1, 0, 0]
], dtype=tf.float32)

# 2. 변수 및 클래스 개수 설정
nb_classes = 3  # 정답의 종류 (A, B, C)
W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight') # 입력 4개 -> 출력 3개
b = tf.Variable(tf.random.normal([nb_classes]), name='bias')

# 3. 최적화 도구 (SGD)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

print("--- 학습 시작 ---")
for step in range(2001):
    with tf.GradientTape() as tape:
        # Softmax: 결과값들을 확률(0~1)로 바꿔주고, 다 합치면 1이 되게 함
        logits = tf.matmul(x_data, W) + b
        hypothesis = tf.nn.softmax(logits)

        cost = tf.reduce_mean(-tf.reduce_sum(y_data * tf.math.log(hypothesis), axis=1))

    # Gradient 계산 및 업데이트
    grads = tape.gradient(cost, [W, b])
    optimizer.apply_gradients(zip(grads, [W, b]))

    if step % 200 == 0:
        print(f"Step: {step:4d}, Cost: {cost.numpy():.4f}")

print("--- 학습 완료 ---")

# ---------------------------------------
# 4. 학습 결과 테스트 (One-hot Encoding -> 실제 값)
# ---------------------------------------
print("\n--- 테스트 결과 ---")
# 가설 계산
result = tf.nn.softmax(tf.matmul(x_data, W) + b)

# tf.argmax: 가장 큰 확률을 가진 인덱스(정답 번호)를 찾음
predict = tf.argmax(result, 1) # 예측값
real_val = tf.argmax(y_data, 1) # 실제값

print("예측값:", predict.numpy())
print("실제값:", real_val.numpy())
print("정확도:", tf.reduce_mean(tf.cast(tf.equal(predict, real_val), dtype=tf.float32)).numpy())
```

```python
--- 학습 시작 ---
Step:    0, Cost: 3.0717
Step:  200, Cost: 0.6063
Step:  400, Cost: 0.4946
Step:  600, Cost: 0.3986
Step:  800, Cost: 0.3064
Step: 1000, Cost: 0.2418
Step: 1200, Cost: 0.2185
Step: 1400, Cost: 0.1993
Step: 1600, Cost: 0.1831
Step: 1800, Cost: 0.1692
Step: 2000, Cost: 0.1573
--- 학습 완료 ---

--- 테스트 결과 ---
예측값: [2 2 2 1 1 1 0 0]
실제값: [2 2 2 1 1 1 0 0]
정확도: 1.0
```

**MNIST Dataset 실습**

**Training epoch/batch**

- **`one epoch`**: one forward pass and pne backward pass of all the training examples
- **`batch size`**: the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you’ll need.
- number of **`iterations`**: number of passes, each pass using [batch_size] number of examples. To be clear, one pass = one forward pass + one backward pass

**Examples**: If you hace 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.

```python
import tensorflow as tf
import numpy as np

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# 2. 데이터 전처리 (Normalization & Flatten)
# 값을 0~255 -> 0~1 사이로 정규화하고, 2차원(28x28)을 1차원(784)으로 폅니다.
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# 정답(Label)을 One-Hot Encoding으로 변환 (예: 5 -> [0,0,0,0,0,1,0,0,0,0])
y_train = tf.one_hot(y_train, depth=10)
y_test = tf.one_hot(y_test, depth=10)

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(100).shuffle(10000)

# 4. 변수 초기화 (W, b) - 반드시 tf.Variable 사용!
# 입력 784개 -> 출력 10개 (0~9 숫자)
W = tf.Variable(tf.random.normal([784, 10]), name='weight')
b = tf.Variable(tf.random.normal([10]), name='bias')

# 최적화 도구
optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)

print("--- 학습 시작 ---")
for epoch in range(15):
    total_cost = 0
    step = 0
    
    # 100개씩 잘린 데이터(batch)를 하나씩 꺼내서 학습
    for batch_xs, batch_ys in train_dataset:
        with tf.GradientTape() as tape:
            # 가설: H(x) = Softmax(Wx + b)
            logits = tf.matmul(batch_xs, W) + b
            hypothesis = tf.nn.softmax(logits)
            
            # Cost: Cross Entropy
            cost = tf.reduce_mean(-tf.reduce_sum(batch_ys * tf.math.log(hypothesis), axis=1))
        
        # 학습 (Gradient 계산 및 업데이트)
        grads = tape.gradient(cost, [W, b])
        optimizer.apply_gradients(zip(grads, [W, b]))
        
        total_cost += cost
        step += 1

    print(f"Epoch: {epoch + 1}, Avg Cost: {total_cost / step:.4f}")

print("--- 학습 완료 ---")

# 5. 정확도 테스트
# 테스트 데이터로 예측 수행
pred_result = tf.nn.softmax(tf.matmul(x_test, W) + b)
correct_prediction = tf.equal(tf.argmax(pred_result, 1), tf.argmax(y_test, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

print(f"최종 정확도(Accuracy): {accuracy.numpy() * 100:.2f}%")
```

```python
--- 학습 시작 ---
Epoch: 1, Avg Cost: 1.2210
Epoch: 2, Avg Cost: 0.6013
Epoch: 3, Avg Cost: 0.5076
Epoch: 4, Avg Cost: 0.4578
Epoch: 5, Avg Cost: 0.4253
Epoch: 6, Avg Cost: 0.4020
Epoch: 7, Avg Cost: 0.3835
Epoch: 8, Avg Cost: 0.3689
Epoch: 9, Avg Cost: 0.3575
Epoch: 10, Avg Cost: 0.3470
Epoch: 11, Avg Cost: 0.3399
Epoch: 12, Avg Cost: 0.3318
Epoch: 13, Avg Cost: 0.3256
Epoch: 14, Avg Cost: 0.3193
Epoch: 15, Avg Cost: 0.3160
--- 학습 완료 ---
최종 정확도(Accuracy): 91.40%
```

**plt.show() 를 활용해 이미지 띄우기**

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import random

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# 2. 데이터 전처리 (Normalization & Flatten)
# 값을 0~255 -> 0~1 사이로 정규화하고, 2차원(28x28)을 1차원(784)으로 폅니다.
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# 정답(Label)을 One-Hot Encoding으로 변환 (예: 5 -> [0,0,0,0,0,1,0,0,0,0])
y_train = tf.one_hot(y_train, depth=10)
y_test = tf.one_hot(y_test, depth=10)

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(100).shuffle(10000)

# 4. 변수 초기화 (W, b) - 반드시 tf.Variable 사용!
# 입력 784개 -> 출력 10개 (0~9 숫자)
W = tf.Variable(tf.random.normal([784, 10]), name='weight')
b = tf.Variable(tf.random.normal([10]), name='bias')

# 최적화 도구
optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)

print("--- 학습 시작 ---")

# 전체 데이터를 15번 반복(Epochs)해서 학습
for epoch in range(15):
    total_cost = 0
    step = 0
    
    # 100개씩 잘린 데이터(batch)를 하나씩 꺼내서 학습
    for batch_xs, batch_ys in train_dataset:
        with tf.GradientTape() as tape:
            # 가설: H(x) = Softmax(Wx + b)
            logits = tf.matmul(batch_xs, W) + b
            # log_softmax를 내부적으로 처리해서 nan이 안 뜨게 막아줍니다.
            cost_i = tf.nn.softmax_cross_entropy_with_logits(labels=batch_ys, logits=logits)
            cost = tf.reduce_mean(cost_i)
        # 학습 (Gradient 계산 및 업데이트)
        grads = tape.gradient(cost, [W, b])
        optimizer.apply_gradients(zip(grads, [W, b]))
        
        total_cost += cost
        step += 1

    print(f"Epoch: {epoch + 1}, Avg Cost: {total_cost / step:.4f}")

print("--- 학습 완료 ---")

# 5. 정확도 테스트
# 테스트 데이터로 예측 수행
pred_result = tf.nn.softmax(tf.matmul(x_test, W) + b)
correct_prediction = tf.equal(tf.argmax(pred_result, 1), tf.argmax(y_test, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

print(f"최종 정확도(Accuracy): {accuracy.numpy() * 100:.2f}%")

# 1. 테스트 데이터 중 랜덤하게 하나를 뽑습니다.
random_idx = random.randint(0, len(x_test) - 1)
selected_image = x_test[random_idx]
selected_label = y_test[random_idx] # One-hot 인코딩된 정답

# 2. 모델에게 물어봅니다 (예측)
# (1, 784) 형태로 모양을 맞춰서 넣어줘야 합니다.
prediction = tf.nn.softmax(tf.matmul(selected_image.reshape(1, 784), W) + b)
predicted_label = tf.argmax(prediction, 1).numpy()[0] # 가장 높은 확률의 숫자
actual_label = tf.argmax(selected_label).numpy()      # 실제 정답 숫자

# 3. 그림으로 보여줍니다 (imshow)
# 784개로 펴진 데이터를 다시 28x28 네모 모양으로 복구해야 그림이 됩니다.
plt.figure(figsize=(5, 5))
plt.imshow(selected_image.reshape(28, 28), cmap='Greys')

plt.title(f"Predicted: {predicted_label} / Actual: {actual_label}")
plt.axis('off') # 불필요한 축 눈금 끄기
plt.show()

# 4. 확률도 같이 출력해보기 (선택사항)
print(f"방금 본 이미지의 예측 결과: {predicted_label}")
print(f"실제 정답: {actual_label}")
```

<img width="499" height="511" alt="image" src="https://github.com/user-attachments/assets/5429dcf4-f0ab-438d-8924-3b21a5d23f91" />
