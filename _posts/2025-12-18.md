---
layout: posts
title: "XOR, Backpropagation (lec 8-9.2)"
categories: ["DeepLearning"]
---

### Deeplearning 의 기본 개념: 시작과 XOR 문제

**Activation Functions :**

어떤 값 이상이면 1 이하면 0 

**XOR 문제**

<img width="631" height="273" alt="image" src="https://github.com/user-attachments/assets/3e558f66-c145-4093-844c-e3a65c58467a" />

**perceptron**

MLP (multilayer perceptrons - multilayer neural nets)

<img width="593" height="331" alt="image" src="https://github.com/user-attachments/assets/1493de55-1580-4e26-84cb-67b04b651855" />

**Backpropagation**

<img width="841" height="247" alt="image" src="https://github.com/user-attachments/assets/09d93b61-d107-4297-a733-a608bcc537c6" />

error → 뒤로 가면서 다시 학습시키자.

Convolutional Neural Networks

<img width="500" height="324" alt="image" src="https://github.com/user-attachments/assets/2b913a56-8088-40e4-bd0e-ce2236e9a7d0" />

(고양이로 실험을 진행했더니 “-” 이 그림을 보여줬을 때 활성화되는 뉴런과 “+” 그림을 보여줬을 때 활성화되는 뉴런이 다르다. ⇒ 눈으로 볼 때 모든 신경이 활성화 되는 것이 아니다.)

<img width="786" height="241" alt="image" src="https://github.com/user-attachments/assets/44e4dc74-e480-4aea-9a50-4fc5d221c4d1" />

한 번에 다 보는 것이 아니라 부분 읽고 나중에 다 합친다.

A Big Problem

- Backpropagation just didn’t work well for normal neural nets with many layers
    - 복잡한 문제를 풀기 위해서는 예를 들어 10개의 layer 가 필요한데 backpropagation 을 사용하면 뒤로 갈수록(앞으로 갈 수록) 의미를 잃게 된다.

Breakthrough

- Neural networks with many layers really could be trained well, if the weights are initailized in a clever way rather than randomly
- Deep machine learning methods are more efficient for difficult pproblems than shallow methods
- Rebranding to Deep Nets, Deep Learning

**Tensor**

axis → 주의

```python
# axios
import tensorflow as tf
x = [[1., 2.],
     [3., 4.]]

print(tf.reduce_mean(x))
# tf.Tensor(2.5, shape=(), dtype=float32)
print(tf.reduce_mean(x, axis = 0))
# tf.Tensor([2. 3.], shape=(2,), dtype=float32)
print(tf.reduce_mean(x, axis = 1))
# tf.Tensor([1.5 3.5], shape=(2,), dtype=float32)
```

```python
# argmax
y = [[0, 1, 2],
     [2, 1, 0]]
print(tf.argmax(x, axis = 0))
# tf.Tensor([1 1], shape=(2,), dtype=int64)
print(tf.argmax(x, axis = 1))
# tf.Tensor([1 1], shape=(2,), dtype=int64)
```

```python
# Tensor Manipulation
import tensorflow as tf
matrix1 = [[1., 2.], [3., 4.]]
matrix2 = [[1.], [2.]]

print("matrix1: ", matrix1)
print("matrix2: ", matrix2)

# 1*1 + 2*2 = 5
# 3*1 + 4*2 = 1
print(tf.matmul(matrix1, matrix2)) # matrix 곱 => broadcasting
```

```python
# reshape
import tensorflow as tf
import numpy as np

x = np.array([[[0, 1, 2],
      [3, 4, 5]],
      [[6, 7, 8],
       [9, 10, 11]]])

print(x.shape)
print(tf.reshape(x, shape = [-1, 3]))
```

```python
(2, 2, 3)
tf.Tensor(
[[ 0  1  2]
 [ 3  4  5]
 [ 6  7  8]
 [ 9 10 11]], shape=(4, 3), dtype=int32)
(2, 2, 3)
```

### Neural Nets(NN) for XOR

하나의 unit 으로는 XOR 을 절대 풀 수 없다.

⇒ 여러 개의 unit 으로는 풀 수 있다.

→ 여러 개의 unit을 backpropagation 으로 w, b 를 어떻게 학습 시킬 것이냐
