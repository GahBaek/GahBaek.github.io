---
layout: posts
title: "데이터 전처리"
categories: ["DeepLearning"]
---


# 데이터 전처리
## 텍스트 전처리

### 토큰화

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import urllib.request
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer  # type: ignore[reportMissingImports]
from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore[reportMissingImports]
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence # type: ignore[reportMissingImports]
import nltk
from nltk.tokenize import word_tokenize
import nltk

sentence = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
print('단어 토큰화1 :', word_tokenize(sentence))
print('단어 토큰화2 :',WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
print('단어 토큰화3 :',text_to_word_sequence("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
```

```python
단어 토큰화1 : ['Do', "n't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', "'s", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
단어 토큰화2 : ['Don', "'", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', "'", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']
단어 토큰화3 : ["don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', "jone's", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']
```

- 구두점이나 특수 문자를 단순 제외해서는 안된다.
- 줄임말과 단어 내에 띄어쓰기가 있는경우

### 정제와 정규화

- 정제
    - 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.
- 정규화
    - 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.
1. 규칙에 기반한 표기가 다른 단어들의 통합
    1. 어간 추출
    2. 표제어 추출
2. 대, 소문자 통합
    1. 소문자 변환
        - 하지만, 무작정 통합되어서는 안된다. (US 와 us 는 다르기 때문이다.)
3. 불필요한 단어의 제거
    1. 등장 빈도가 적은 단어
    2. 길이가 짧은 단어
        1. 영어권에서는 단어의 길이가 짧은 단어를 삭제하면 크게 의미를 갖지 못하는 단어를 줄이는 효과가 있다.
4. 정규 표현식
    1. HTML 문서의 경우 태그
    2. 뉴스 기사를 크롤링한다면 시간 등
    3. 규칙에 기반하여 한번에 제거하는 방식으로 정규 표현식은 매우 유용하다.

### 어간 추출과 표제어 추출

- 표제어 추출
    - 어간: 단어의 의미를 담고 있는 단어의 핵심 부분
    - 접사: 단어에 추가적인 의미를 주는 부분
    
    ⇒ 형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말한다.
    
    - Steaming (어간 추출)
        
        am → am
        
        the going → the go
        
        having → hav
        
    - Lemmatization (표제어 추출)
        
        am → be
        
        the going → the going
        
        having → have
        

### 불용어 (Stopword)

갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요하다. 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들.

```python
example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english')) 

word_tokens = word_tokenize(example)

result = []
for word in word_tokens: 
    if word not in stop_words: 
        result.append(word) 

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result)
```

### 정수 인코딩

### 패딩

자연어 처리 과정에서 각 문장의 길이는 서로 다를 수 있다. 하지만 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고 한꺼번에 묶어서 처리할 수 있다. 쉽게 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있다. 이러한 작업은 Numpy 로 패딩하는 방법과 Keras 전처리 도구로 패딩하는 방법이 있다.

가장 길이가 긴 문장의 길이를 K 라고 할 때

K 를 기준으로 패딩을 하게 된다면 문장의 길이가 K 보다 짧다면 해당 문장의 뒤에는 숫자 0이 붙어서 모든 문장의 길이가 전부 K가 된다. 

기계는 이것들을 하나의 행렬로 보고 **`병렬 처리`**를 할 수 있다.

이와 같이 데이터에 특정 값을 채워서 데이터의 크기를 조정하는 것을 패딩이라고 한다. 

Numpy 로 패딩을 할 경우 0 이 뒤에 채워진다. 반면에 Keras 의 경우 기본적으로 문서의 앞을 0으로 채운다.

길이가 K 보다 짧은 문서는 0으로 패딩되고 기존에 K 보다 길었다면 데이터가 손실된다.

### 원-핫 인코딩

컴퓨터 또는 기계는 문자보다 숫자를 더 잘 처리할 수 있다. 이를 위해 자연어 처리에서는 문자를 숫자로 바꾸는 여러 가지 기법이 있다.

원-핫 인코딩은 그 많은 기법 중에서 단어를 표현하는 가장 기본적인 표현 방법이다.

- one-hot encoding
    - 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다.
    - 과정은 다음과 같다.
        - 정수 인코딩을 수행한다.
        - 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고 다른 단어의 인덱스의 위치에는 0을 부여한다.
    - 장점
        - 단순하고 직관적
        - 순서 정보를 넣지 않는다.
    - 단점
        - **차원이 너무 커진다.** (고차원 + 희소 벡터)
        - 단어 사이의 유**사도 정보가 없다.**
        
        ⇒ 최근에는 임베딩을 더 사용한다.
