---
layout: posts
title: "모두를 위한 머신러닝 (lec1-3)"
categories: ["DeepLearning"]
---


### 머신 러닝이란,

**Supervised / Unsupervised learning**

- Supervised Learning: labeled dataset 으로 학습하는 것
- Unsupervised Learning: 데이터의 구조를 학습한다. 라벨이 지정되지 않는 데이터를 학습한다.

**Types of supervised learning**

- Regression (1~100)
- Binary Classification (P/NP)
- Multi-label Classification (A, B, C … F)

### Linear Regression

**Cost Function**

$H(x) - y$

→ 음수가 될 수 있어 좋지 않다.

$(H(x) - y)^2$

모든 데이터에 대한 평균

<img width="387" height="120" alt="image" src="https://github.com/user-attachments/assets/6cf70230-ff93-49a8-895f-6899249287eb" />


<img width="1029" height="402" alt="image" src="https://github.com/user-attachments/assets/1663b908-c1f1-4c7d-9b9d-28ce45b82964" />


### Linear Regression Cost 함수 최소화

**Gradient Descent algorithm**

- minimize cost function
- Gradient descent is used many minimization problems
- For a given cost function, cost(W, b), it will find W, b to minimize cost
- It can be appliued to more general function (w1, w2 …)

<img width="607" height="170" alt="image" src="https://github.com/user-attachments/assets/4b1fe924-46f7-47d6-a818-4b5afdc4e119" />


**Convex Function**

: 어느 점에서 시작하던 간에 하나의 값으로 수렴하는 것을 보장한다.

cost function 설계시 모양이 convex function 인지 반드시 확인해야한다.  
<img width="300" height="209" alt="image" src="https://github.com/user-attachments/assets/a249e749-b38e-4984-bbc6-2713ed8b8975" />

```python
import tensorflow as tf

# 1. placeholder 대신 그냥 변수에 값을 담습니다. (Tensor 객체 생성)
a = tf.constant(3.0)
b = tf.constant(4.5)

# 2. Session을 열 필요 없이, 바로 더하면 됩니다.
result = a + b

# 3. 결과 확인
print(result) 
# 결과: tf.Tensor(7.5, shape=(), dtype=float32)

print("순수 값만 보기:", result.numpy()) 
# 결과: 7.5
```

### Linear Regression

- Keras 는 Python 으로 만든 스크립트된 **신경망 라이브러리**이다.
- Tensorflow 는 **심층 신경망**에서 실행을 위해 특별히 설계되었다.
- PyTorch는 **자연어 처리**에 사용되는 기계어 학습이다.

```python
import tensorflow as tf

x_train = [1.0, 2.0, 3.0]
y_train = [1.0, 2.0, 3.0]

W = tf.Variable(tf.random.normal([1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

print("--- 학습 시작 ---")
for step in range(1001): # 1000번 반복
    with tf.GradientTape() as tape:
        hypothesis = x_train * W + b
        # 오차 제곱한 뒤 평균 내기.
        cost = tf.reduce_mean(tf.square(hypothesis - y_train))

    # tape 을 이용해 Gradient (기울기) 를 구한다.
    grads = tape.gradient(cost, [W, b])
    
    # optimizer 를 이용해 W 와 b를 업데이트한다.
    optimizer.apply_gradients(zip(grads, [W, b]))

    if step % 100 == 0:
        print(f"Step: {step}, Cost: {cost.numpy():.4f}, Weight: {W.numpy()[0]:.4f}, bias: {b.numpy()[0]:.4f}")

print("--- 학습 완료 ---")
```

```python
--- 학습 시작 ---
Step: 0, Cost: 1.4103, Weight: 0.1888, bias: 0.7840
Step: 100, Cost: 0.0814, Weight: 0.6694, bias: 0.7514
Step: 200, Cost: 0.0503, Weight: 0.7401, bias: 0.5907
Step: 300, Cost: 0.0311, Weight: 0.7957, bias: 0.4643
Step: 400, Cost: 0.0192, Weight: 0.8394, bias: 0.3650
Step: 500, Cost: 0.0119, Weight: 0.8738, bias: 0.2869
Step: 600, Cost: 0.0073, Weight: 0.9008, bias: 0.2256
Step: 700, Cost: 0.0045, Weight: 0.9220, bias: 0.1773
Step: 800, Cost: 0.0028, Weight: 0.9387, bias: 0.1394
Step: 900, Cost: 0.0017, Weight: 0.9518, bias: 0.1096
Step: 1000, Cost: 0.0011, Weight: 0.9621, bias: 0.0861
--- 학습 완료 ---
```

```python
# testing

# 1. 테스트할 새로운 데이터 준비 (학습 때 안 쓴 숫자들)
x_test = [4.0, 5.0, 6.0, 7.0]

# 2. 예측하기 (모델 식에 대입: H(x) = Wx + b)
# W와 b는 이미 위에서 학습을 통해 업데이트된 상태입니다.
y_predict = x_test * W + b

# 3. 결과 출력
print("--- 테스트 결과 ---")
print("입력값(x):", x_test)
print("예측값(y):", y_predict.numpy())
```

```python
--- 테스트 결과 ---
입력값(x): [4.0, 5.0, 6.0, 7.0]
예측값(y): [4.102872  5.110242  6.1176114 7.124981 ]
```

```python
import tensorflow as tf

x_data = [1., 2., 3.]
y_data = [1., 2., 3.]

W = tf.Variable(tf.random.normal(()), name='weight')

learning_rate = 0.1

print("--- 학습 시작 ---")

for step in range(21):
    hypothesis = W * x_data 
    
    cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    # (3) 미분(Gradient) 직접 계산
    # 공식: mean((W * X - Y) * X)
    gradient = tf.reduce_mean((W * x_data - y_data) * x_data)

    # (4) W 업데이트 (Descent)
    # W = W - learning_rate * gradient
    W.assign_sub(learning_rate * gradient)

    # 결과 출력
    print(f"Step: {step}, Cost: {cost.numpy():.4f}, Weight: {W.numpy():.4f}")

print("--- 학습 완료 ---")
```
